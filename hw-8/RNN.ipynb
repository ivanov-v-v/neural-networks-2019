{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 100\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.80427360534668\n",
      "Clip gradient :  3.09128939323171\n",
      "2.266852855682373\n",
      "Clip gradient :  1.3085128250703808\n",
      "0.5177130699157715\n",
      "Clip gradient :  0.5005246828965925\n",
      "0.11306381225585938\n",
      "Clip gradient :  0.5510382840644107\n",
      "0.04264259338378906\n",
      "Clip gradient :  0.4765328277672062\n",
      "0.013887405395507812\n",
      "Clip gradient :  0.038717521105102674\n",
      "0.010044097900390625\n",
      "Clip gradient :  0.0351740218886201\n",
      "0.00825357437133789\n",
      "Clip gradient :  0.0327982868689374\n",
      "0.007105350494384766\n",
      "Clip gradient :  0.014760617359659752\n",
      "0.006451129913330078\n",
      "Clip gradient :  0.007339675786386952\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__()        \n",
    "        self.x2f = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2f = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2i = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2i = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2o = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2o = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2c = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2c = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.filter = nn.Sigmoid()\n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_cell):\n",
    "        f = self.filter(self.x2f(x) + self.h2f(prev_hidden))\n",
    "        i = self.filter(self.x2i(x) + self.h2i(prev_hidden))\n",
    "        o = self.filter(self.x2o(x) + self.h2o(prev_hidden))\n",
    "        c = f * prev_cell + i * self.activation(self.x2c(x) + self.h2c(prev_hidden))\n",
    "        h = o * self.activation(prev_cell)\n",
    "        output = self.outweight(h)\n",
    "        return output, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = LSTM(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 1000\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.90705871582031\n",
      "Clip gradient :  3.340104779019206\n",
      "67.43881225585938\n",
      "Clip gradient :  2.7514992094107193\n",
      "50.07145309448242\n",
      "Clip gradient :  4.8656130141790435\n",
      "44.645477294921875\n",
      "Clip gradient :  24.44931715322141\n",
      "37.02177047729492\n",
      "Clip gradient :  10.648355311751839\n",
      "37.54615783691406\n",
      "Clip gradient :  6.722420443725721\n",
      "37.56373977661133\n",
      "Clip gradient :  13.326107102965453\n",
      "31.884220123291016\n",
      "Clip gradient :  5.784481710351572\n",
      "32.10682678222656\n",
      "Clip gradient :  9.415746504278022\n",
      "30.88884925842285\n",
      "Clip gradient :  12.31549572037178\n",
      "31.32321548461914\n",
      "Clip gradient :  9.827352668472432\n",
      "29.677268981933594\n",
      "Clip gradient :  3.690425230529532\n",
      "26.338966369628906\n",
      "Clip gradient :  1.6452839715283352\n",
      "24.119735717773438\n",
      "Clip gradient :  1.281301606494058\n",
      "22.136348724365234\n",
      "Clip gradient :  1.0590141019236525\n",
      "20.088558197021484\n",
      "Clip gradient :  0.6338464169387046\n",
      "18.277698516845703\n",
      "Clip gradient :  1.2238308503719382\n",
      "16.095157623291016\n",
      "Clip gradient :  1.591925297486606\n",
      "14.18514347076416\n",
      "Clip gradient :  0.8399472024416511\n",
      "12.081022262573242\n",
      "Clip gradient :  0.717745167291754\n",
      "10.435035705566406\n",
      "Clip gradient :  0.5014315924254044\n",
      "9.543803215026855\n",
      "Clip gradient :  1.6075115121594472\n",
      "8.997993469238281\n",
      "Clip gradient :  1.0310429720056662\n",
      "9.795404434204102\n",
      "Clip gradient :  26.928351508757107\n",
      "8.87631607055664\n",
      "Clip gradient :  1.48740686946133\n",
      "8.470199584960938\n",
      "Clip gradient :  1.572476672479711\n",
      "7.771579265594482\n",
      "Clip gradient :  0.7381108508645977\n",
      "7.046430587768555\n",
      "Clip gradient :  0.3129189714911425\n",
      "6.504140853881836\n",
      "Clip gradient :  0.326014612962537\n",
      "6.081912994384766\n",
      "Clip gradient :  0.21570018641770042\n",
      "5.733129501342773\n",
      "Clip gradient :  0.18657140842369263\n",
      "5.426736831665039\n",
      "Clip gradient :  0.17357980495407316\n",
      "5.162857532501221\n",
      "Clip gradient :  0.15266497520994035\n",
      "4.959893703460693\n",
      "Clip gradient :  0.13675826008583178\n",
      "4.787066459655762\n",
      "Clip gradient :  0.12333688963138528\n",
      "4.638840675354004\n",
      "Clip gradient :  0.11402356720094044\n",
      "4.509737014770508\n",
      "Clip gradient :  0.10718397386507894\n",
      "4.402753829956055\n",
      "Clip gradient :  0.7152690433665048\n",
      "4.358443260192871\n",
      "Clip gradient :  0.6937196939478445\n",
      "4.251862525939941\n",
      "Clip gradient :  0.4896441187204117\n",
      "4.161745071411133\n",
      "Clip gradient :  0.1598613533799235\n",
      "4.081150054931641\n",
      "Clip gradient :  0.1052625365250388\n",
      "4.006957054138184\n",
      "Clip gradient :  0.09071686996769404\n",
      "3.935490608215332\n",
      "Clip gradient :  0.10561406469418318\n",
      "3.855679512023926\n",
      "Clip gradient :  0.11028721110903189\n",
      "3.7009963989257812\n",
      "Clip gradient :  0.19147016048232096\n",
      "3.439798355102539\n",
      "Clip gradient :  0.21935589864149957\n",
      "3.1525049209594727\n",
      "Clip gradient :  0.3549053610009533\n",
      "3.1212453842163086\n",
      "Clip gradient :  2.091136615664888\n",
      "2.8858680725097656\n",
      "Clip gradient :  0.4046829657528287\n",
      "2.765890121459961\n",
      "Clip gradient :  0.21049370256203287\n",
      "2.678219795227051\n",
      "Clip gradient :  0.174840684933287\n",
      "2.6051673889160156\n",
      "Clip gradient :  0.17473310684159116\n",
      "2.546067237854004\n",
      "Clip gradient :  0.11722996950539397\n",
      "2.4919118881225586\n",
      "Clip gradient :  0.21169142691322254\n",
      "2.4416379928588867\n",
      "Clip gradient :  0.26502577187197485\n",
      "2.400083541870117\n",
      "Clip gradient :  0.10622861383309712\n",
      "2.3575448989868164\n",
      "Clip gradient :  0.28540567979279474\n",
      "2.335524559020996\n",
      "Clip gradient :  1.4974458730289484\n",
      "2.2935562133789062\n",
      "Clip gradient :  0.19845846124084918\n",
      "2.247241973876953\n",
      "Clip gradient :  0.41446455686883066\n",
      "2.1942996978759766\n",
      "Clip gradient :  0.2839585720691488\n",
      "2.132290840148926\n",
      "Clip gradient :  0.20384652880837056\n",
      "2.0364227294921875\n",
      "Clip gradient :  0.2974255641901357\n",
      "1.8716354370117188\n",
      "Clip gradient :  1.025195445547246\n",
      "1.610677719116211\n",
      "Clip gradient :  0.899802637323328\n",
      "1.4719085693359375\n",
      "Clip gradient :  0.8459810295278202\n",
      "1.3786735534667969\n",
      "Clip gradient :  1.0890606144094366\n",
      "1.291020393371582\n",
      "Clip gradient :  0.4485575733686537\n",
      "1.2285118103027344\n",
      "Clip gradient :  0.26685888311033407\n",
      "1.1770410537719727\n",
      "Clip gradient :  0.14925533432971522\n",
      "1.132817268371582\n",
      "Clip gradient :  0.11565004863020674\n",
      "1.0932416915893555\n",
      "Clip gradient :  0.07606004641067607\n",
      "1.0573005676269531\n",
      "Clip gradient :  0.07054492203988115\n",
      "1.0240211486816406\n",
      "Clip gradient :  0.058709214996865045\n",
      "0.9928255081176758\n",
      "Clip gradient :  0.055138030026418094\n",
      "0.9631690979003906\n",
      "Clip gradient :  0.05479169973869829\n",
      "0.9343652725219727\n",
      "Clip gradient :  0.05419799677409601\n",
      "0.9052762985229492\n",
      "Clip gradient :  0.05590363587474763\n",
      "0.873133659362793\n",
      "Clip gradient :  0.0631771684507506\n",
      "0.8310422897338867\n",
      "Clip gradient :  0.0782963746654642\n",
      "0.7740097045898438\n",
      "Clip gradient :  0.08193584051236275\n",
      "0.7188596725463867\n",
      "Clip gradient :  0.07474174707745174\n",
      "0.6719980239868164\n",
      "Clip gradient :  0.0633044644810166\n",
      "0.6346492767333984\n",
      "Clip gradient :  0.055809087425469375\n",
      "0.6044168472290039\n",
      "Clip gradient :  0.05011924027045646\n",
      "0.579554557800293\n",
      "Clip gradient :  0.045838059080829184\n",
      "0.5586519241333008\n",
      "Clip gradient :  0.04247747697413511\n",
      "0.5406007766723633\n",
      "Clip gradient :  0.04030839075733055\n",
      "0.5246191024780273\n",
      "Clip gradient :  0.038348933712435467\n",
      "0.5102167129516602\n",
      "Clip gradient :  0.05153910988150658\n",
      "0.49982261657714844\n",
      "Clip gradient :  0.4379785303644659\n",
      "0.48593711853027344\n",
      "Clip gradient :  0.2511839493708933\n",
      "0.47388172149658203\n",
      "Clip gradient :  0.12198373731497492\n",
      "0.4630260467529297\n",
      "Clip gradient :  0.09810699839294222\n",
      "0.452911376953125\n",
      "Clip gradient :  0.06263085188629602\n",
      "0.44341182708740234\n",
      "Clip gradient :  0.03004002597305015\n",
      "0.43447208404541016\n",
      "Clip gradient :  0.03856352068510612\n",
      "0.4259653091430664\n",
      "Clip gradient :  0.033058413708955334\n",
      "0.4178791046142578\n",
      "Clip gradient :  0.030813306070610315\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.outweight.in_features)\n",
    "    cc = torch.zeros(rnn.outweight.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "        y, hh, cc = rnn(x, hh, cc)\n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.h2f.in_features)\n",
    "cc = torch.zeros(rnn.h2f.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh, cc = rnn(x, hh, cc)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()        \n",
    "        self.x2z = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2z = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2r = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2r = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.x2h = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.h2h = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.filter = nn.Sigmoid()\n",
    "        self.activation  = nn.Tanh()\n",
    "        \n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        z = self.filter(self.x2z(x) + self.h2z(prev_hidden))\n",
    "        r = self.filter(self.x2r(x) + self.h2r(prev_hidden))\n",
    "        h = (1 - z) * prev_hidden + z * self.activation(self.x2h(x) + self.h2h(r * prev_hidden))\n",
    "        output = self.outweight(h)\n",
    "        return output, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = GRU(in_size=ds.vec_size, hidden_size=10, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 500\n",
    "optim     = SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.47886657714844\n",
      "Clip gradient :  4.246459507391692\n",
      "57.589637756347656\n",
      "Clip gradient :  6.138605055155907\n",
      "31.48200225830078\n",
      "Clip gradient :  7.74738598008901\n",
      "12.11317253112793\n",
      "Clip gradient :  3.8863980785581145\n",
      "3.0512280464172363\n",
      "Clip gradient :  2.2431033866148873\n",
      "1.185276985168457\n",
      "Clip gradient :  29.5626127754399\n",
      "4.8191680908203125\n",
      "Clip gradient :  11.85294712203797\n",
      "2.091378688812256\n",
      "Clip gradient :  3.1700331771126793\n",
      "0.32323551177978516\n",
      "Clip gradient :  0.5315802917414627\n",
      "0.16036510467529297\n",
      "Clip gradient :  2.3423824213479034\n",
      "0.10284137725830078\n",
      "Clip gradient :  0.10512959827584963\n",
      "0.07022953033447266\n",
      "Clip gradient :  0.05610087464148777\n",
      "0.05261516571044922\n",
      "Clip gradient :  0.030949785757231695\n",
      "0.043877601623535156\n",
      "Clip gradient :  0.023898943943510432\n",
      "0.038496971130371094\n",
      "Clip gradient :  0.020640324013717532\n",
      "0.03451728820800781\n",
      "Clip gradient :  0.017309181605796718\n",
      "0.031485557556152344\n",
      "Clip gradient :  0.015593723406756948\n",
      "0.029036521911621094\n",
      "Clip gradient :  0.014342514259750777\n",
      "0.026975631713867188\n",
      "Clip gradient :  0.013234513866553796\n",
      "0.025214195251464844\n",
      "Clip gradient :  0.012340796037700774\n",
      "0.02368450164794922\n",
      "Clip gradient :  0.011576170935003332\n",
      "0.022339820861816406\n",
      "Clip gradient :  0.01090527232706468\n",
      "0.021148681640625\n",
      "Clip gradient :  0.010317192087985778\n",
      "0.020081520080566406\n",
      "Clip gradient :  0.009793338318904539\n",
      "0.01912403106689453\n",
      "Clip gradient :  0.009324230513849494\n",
      "0.01824951171875\n",
      "Clip gradient :  0.008897390564268776\n",
      "0.017457008361816406\n",
      "Clip gradient :  0.008512516516879395\n",
      "0.01673126220703125\n",
      "Clip gradient :  0.008160626099435757\n",
      "0.01606464385986328\n",
      "Clip gradient :  0.007838277523183322\n",
      "0.015448570251464844\n",
      "Clip gradient :  0.007540827632354823\n",
      "0.014883041381835938\n",
      "Clip gradient :  0.007268157921459129\n",
      "0.014352798461914062\n",
      "Clip gradient :  0.007013454875764936\n",
      "0.013861656188964844\n",
      "Clip gradient :  0.006776565413653644\n",
      "0.013401031494140625\n",
      "Clip gradient :  0.006556008177616102\n",
      "0.012974739074707031\n",
      "Clip gradient :  0.006351000873302039\n",
      "0.012567520141601562\n",
      "Clip gradient :  0.006155813367522293\n",
      "0.012189865112304688\n",
      "Clip gradient :  0.005974909639549007\n",
      "0.011832237243652344\n",
      "Clip gradient :  0.005803547531027593\n",
      "0.01149749755859375\n",
      "Clip gradient :  0.005643079029820843\n",
      "0.011178970336914062\n",
      "Clip gradient :  0.005490390982444279\n",
      "0.010876655578613281\n",
      "Clip gradient :  0.005345490460119382\n",
      "0.010593414306640625\n",
      "Clip gradient :  0.0052095521346180165\n",
      "0.01032257080078125\n",
      "Clip gradient :  0.005079326236101313\n",
      "0.010061264038085938\n",
      "Clip gradient :  0.004954816679067067\n",
      "0.00981903076171875\n",
      "Clip gradient :  0.0048385006284711336\n",
      "0.009584426879882812\n",
      "Clip gradient :  0.004725699612161473\n",
      "0.009359359741210938\n",
      "Clip gradient :  0.004617600644134666\n",
      "0.009146690368652344\n",
      "Clip gradient :  0.0045156711181649975\n",
      "0.008942604064941406\n",
      "Clip gradient :  0.004417709316856695\n",
      "0.008748054504394531\n",
      "Clip gradient :  0.0043242370349612785\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.outweight.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "        y, hh = rnn(x, hh)\n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.h2z.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
